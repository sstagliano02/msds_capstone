{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate #for pretty printing of df\n",
    "\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import itertools  #used for flattening lists of lists\n",
    "import math\n",
    "import csv\n",
    "from help_functions import test_dictionary #for quickly pretty dictionary\n",
    "from help_functions import get_next_qtr #for getting next quarter given a quarter/year\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stop Words, CSV Files, and Year List, SNL_Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words: ['about', 'so', 'over', 'should', 'a', 'mightn', \"haven't\", 'shan', 'wasn', 'how', 'his', 'in', 'up', 'then', 'couldn', \"you're\", 'further', \"won't\", 'their', 'if', 'above', 'both', 'an', 'him', 'here', 'he', 'having', 'after', 'such', 'you', 'has', \"hadn't\", 'ourselves', 'who', 's', 'me', \"shouldn't\", \"hasn't\", 'did', 'to', 'd', 'now', 'itself', 'other', 'same', 'i', 't', 'under', 'have', 'that', 'yours', 'm', 'during', \"you've\", \"you'd\", 'it', 'these', 'which', 'wouldn', 'your', \"doesn't\", 'needn', 'doing', 'than', 'before', 'from', 'y', 'what', 'and', 'she', 'can', 'there', 'was', \"shan't\", 'didn', 'each', \"she's\", \"don't\", 'my', 'just', 'myself', 'does', 'once', \"aren't\", 'doesn', 'those', 'ain', \"mustn't\", 'the', 'don', 'by', 'down', 'or', 'most', 'they', 'all', 'hers', 'am', 'on', 'with', 'will', 'own', 'while', 'some', 'through', 'too', 'hadn', 're', \"that'll\", 'but', 'its', 'are', 'against', \"weren't\", 'no', 'aren', \"isn't\", 'more', 'them', 'shouldn', \"should've\", 'do', 'had', 'again', 'been', 'why', 'as', 'at', 'ma', 'won', \"couldn't\", 'themselves', 'be', 'for', 'very', 'where', 'off', 'her', \"wouldn't\", 'until', 'not', 'yourself', 'between', 'weren', \"didn't\", 'when', 'few', 'this', 'herself', 've', 'yourselves', \"you'll\", 'out', 'o', 'of', 'any', 'into', 'we', 'isn', 'll', 'is', 'being', 'ours', 'because', \"mightn't\", \"wasn't\", \"it's\", \"needn't\", 'our', 'whom', 'mustn', 'were', 'only', 'nor', 'theirs', 'hasn', 'himself']\n",
      "\n",
      "+---+---------------+------------+-----------+---------+--------+--------------------+-------------------+-------------+------------+--------------+-----------+\n",
      "|   | transcript_id |   ciq_id   |  snl_id   | quarter |  year  | total_asset_before | total_asset_after | loan_before | loan_after |  l2a_delta   | loan_diff |\n",
      "+---+---------------+------------+-----------+---------+--------+--------------------+-------------------+-------------+------------+--------------+-----------+\n",
      "| 0 |   1790209.0   | 13314302.0 | 4055785.0 | Q4 2019 | 2019.0 |     12269288.0     |    12159919.0     | 10240434.0  | 10500284.0 | 0.028876319  | 259850.0  |\n",
      "| 1 |   484381.0    | 2387628.0  | 4047200.0 | Q3 2012 | 2012.0 |     16509440.0     |    18242878.0     | 11459931.0  | 14593134.0 | 0.105791815  | 3133203.0 |\n",
      "| 2 |   1177788.0   |  349262.0  | 102420.0  | Q3 2017 | 2017.0 |     1763750.0      |     1776911.0     |  1465917.0  | 1469842.0  | -0.003947067 |  3925.0   |\n",
      "| 3 |   1759737.0   |  302423.0  | 100425.0  | Q3 2017 | 2017.0 |     5340299.0      |     5810129.0     |  3414438.0  | 3841682.0  | 0.021832187  | 427244.0  |\n",
      "| 4 |   1441350.0   |  430453.0  | 4054569.0 | Q1 2018 | 2018.0 |     4369100.0      |     4221874.0     |  2066393.0  | 2164903.0  |  0.03982626  |  98510.0  |\n",
      "+---+---------------+------------+-----------+---------+--------+--------------------+-------------------+-------------+------------+--------------+-----------+\n",
      "\n",
      "mapping files have been loaded in\n"
     ]
    }
   ],
   "source": [
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "words_to_remove = ['below', 'haven']   #reasonable words to be meaningful\n",
    "stop_words = list(filter(lambda word: word not in words_to_remove, nltk_stop_words))\n",
    "print(f'stop_words: {stop_words}')\n",
    "\n",
    "### create list of csv files and years\n",
    "csv_files = [os.path.join('data', file) for file in os.listdir('data')]\n",
    "years_list = list(range(2009, 2025))\n",
    "\n",
    "### create a look up file to get ciq id and map to related snl id\n",
    "seed_words = ['uncertainty','uncertain','ambiguity', 'confusion', 'unpredictable', 'unpredictability']\n",
    "\n",
    "### import csv as dataframes for transcript/company ids and loan data/snl ids\n",
    "delta_df = pd.read_csv('snldata/transcript_loans.csv', encoding='utf-8')\n",
    "print()\n",
    "print(tabulate(delta_df.head(), headers='keys', tablefmt='pretty'))\n",
    "\n",
    "print('')\n",
    "print(f'mapping files have been loaded in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soto uncertainty words: ['adapted', 'amid', 'amidst', 'amplified', 'anxiety', 'attacks', 'austerity', 'backdrop', 'benign', 'bipartisan', 'brexit', 'ceiling', 'challenges', 'challenging', 'cliff', 'climate', 'clouded', 'commonwealth', 'concerns', 'conditions', 'confluence', 'confronting', 'congress', 'consumption', 'crash', 'crises', 'currents', 'cycles', 'deficit', 'deficits', 'deflation', 'deflationary', 'downturn', 'dysfunction', 'economic', 'election', 'elections', 'emerged', 'encountered', 'environment', 'environments', 'eu', 'euro', 'eurozone', 'face', 'faced', 'faces', 'facing', 'fears', 'fiscal', 'flash', 'fragile', 'franc', 'geo', 'geopolitical', 'governmental', 'governments', 'gridlock', 'gyrations', 'hampering', 'headwinds', 'heightened', 'illiquidity', 'immune', 'impasse', 'instability', 'intervention', 'iraq', 'lackluster', 'legislative', 'legislature', 'lingering', 'looming', 'ltro', 'macroeconomic', 'makers', 'midst', 'midterm', 'monetary', 'myriad', 'nafta', 'navigate', 'navigated', 'navigating', 'paralysis', 'persist', 'persisted', 'persistent', 'persistently', 'persists', 'peso', 'political', 'posed', 'presidential', 'prevailed', 'prevailing', 'prolonged', 'protracted', 'psychology', 'reactions', 'realities', 'recessionary', 'referendum', 'reforms', 'rhetoric', 'rican', 'ripple', 'sars', 'sequester', 'shutdown', 'sluggish', 'society', 'sparked', 'spite', 'stimulative', 'stimulus', 'stressful', 'struggles', 'surrounding', 'swiss', 'tariff', 'tariffs', 'tensions', 'terrorism', 'terrorist', 'threat', 'threats', 'tsunami', 'tumultuous', 'turbulence', 'turbulent', 'turmoil', 'uncertain', 'uncertainty', 'uneven', 'unfolded', 'unprecedented', 'unrest', 'unsettled', 'unstable', 'upheaval', 'war', 'weathered', 'weathering', 'withstand']\n"
     ]
    }
   ],
   "source": [
    "with open('snldata/soto_uncertainty_words.txt', 'r') as file:\n",
    "    # Read each line and strip the newline characters\n",
    "    soto_uncertainty_words = [line.strip() for line in file.readlines()]\n",
    "\n",
    "print(f'soto uncertainty words: {soto_uncertainty_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definition for Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create objects to store data for each year and all years combined\n",
    "\n",
    "\n",
    "class Nodes:\n",
    "  node_dict = {} #tracks nodes for each year\n",
    "\n",
    "  def __init__(self, year, lem_dict, flat_lem_dict, lem_corpus, flat_lem_corpus, model=None, uncertainty_wordlist=None):\n",
    "\n",
    "    self.year = year\n",
    "    self.model = model\n",
    "    self.uncertainty_wordlist = uncertainty_wordlist  #tracks final uncertainty wordlist\n",
    "    self.lem_corpus = lem_corpus              #stores the cleaned corpus with bigrams for running model (list of sentences)\n",
    "    self.lem_dict = lem_dict                    #dictionary storing transcript_id:list of sentences for scoring\n",
    "    self.flat_lem_corpus = flat_lem_corpus    #stores flattened list of corpus bigrams for idf calculation\n",
    "    self.flat_lem_dict = flat_lem_dict\n",
    "\n",
    "    Nodes.node_dict[year] = self\n",
    "\n",
    "#tracks combined node\n",
    "class Combined_Node:\n",
    "\n",
    "  def __init__(self, lem_dict, flat_lem_dict, lem_corpus, flat_lem_corpus, model=None, uncertainty_wordlist=None):\n",
    "\n",
    "    self.model = model\n",
    "    self.uncertainty_wordlist = uncertainty_wordlist\n",
    "    self.lem_corpus = lem_corpus              #stores the cleaned corpus with bigrams for running model (list of sentences)\n",
    "    self.lem_dict = lem_dict                    #dictionary storing transcript_id:list of sentences for scoring\n",
    "    self.flat_lem_corpus = flat_lem_corpus\n",
    "    self.flat_lem_dict = flat_lem_dict    #stores flattened list of corpus bigrams for idf calculation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take raw text and tokenize the words\n",
    "def split_text_to_sentences_words(text):\n",
    "    # Split into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Split each sentence into a list of words\n",
    "    sentences_words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return sentences_words\n",
    "\n",
    "#grab pos for lemmatizing\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes single csv file and returns dictionary transcipt_id: [[word, word, word],[word, word, word]]\n",
    "def get_corpus(csv_file):\n",
    "   df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "   df['COMPONENTTEXT_SPLIT'] = df['COMPONENTTEXT'].apply(split_text_to_sentences_words) #splits data in pandas cell into list of sentences\n",
    "\n",
    "   unprocessed_dict = {}\n",
    "   for row in df.itertuples():\n",
    "      if row.TRANSCRIPTID not in unprocessed_dict:\n",
    "         unprocessed_dict[row.TRANSCRIPTID] = row.COMPONENTTEXT_SPLIT\n",
    "      else:\n",
    "         unprocessed_dict[row.TRANSCRIPTID].extend(row.COMPONENTTEXT_SPLIT)\n",
    "   return unprocessed_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes unprocessed dict and returns processed dict (lowercase, lemmatize with pos, remove punct, leave hyphens)\n",
    "\n",
    "def clean(tokenized_dict):\n",
    "   num_tokens_before = 0\n",
    "   num_tokens_after = 0\n",
    "   punct_removed_dict = {}\n",
    "\n",
    "   for transcriptid, text in tokenized_dict.items():\n",
    "      p_text = []\n",
    "      for sentence in text:\n",
    "         p_sentence = []\n",
    "         for word in sentence:\n",
    "            num_tokens_before += 1\n",
    "            p_word = word.lower()                                  #lowercase the text\n",
    "            p_word = re.sub(r'(?<!\\w)-(?!\\w)|[^\\w\\s-]', '', p_word) #remove punctuation but keep hyphens\n",
    "            if len(p_word) > 0:\n",
    "               p_sentence.append(p_word)\n",
    "         p_text.append(p_sentence)\n",
    "         num_tokens_after += len(p_text)\n",
    "      punct_removed_dict[transcriptid] = p_text\n",
    "   return punct_removed_dict, num_tokens_before, num_tokens_after\n",
    "\n",
    "def create_bigrams_pre_lem(punct_removed_dict, min_count=10, threshold = 100):\n",
    "   flat_pr_corpus = [item for sublist in punct_removed_dict.values() for item in sublist]\n",
    "\n",
    "   phrases = Phrases(flat_pr_corpus, min_count, threshold, scoring='default')\n",
    "   bigram_phraser = Phraser(phrases)\n",
    "\n",
    "   num_tokens = 0\n",
    "   bigram_dict = {}\n",
    "\n",
    "   for transcript_id, text in punct_removed_dict.items():\n",
    "      bigram_text = [bigram_phraser[sentence] for sentence in text] #creates a list of list with bigrams included\n",
    "      bigram_dict[transcript_id] = bigram_text\n",
    "      num_tokens += sum(len(sublist) for sublist  in bigram_text)\n",
    "\n",
    "   return phrases, bigram_phraser, bigram_dict, num_tokens\n",
    "\n",
    "def process_the_data_lemmatizer(bigram_dict, stop_words):\n",
    "   num_tokens_after = 0\n",
    "   lem_dict = {}\n",
    "   flat_lem_dict = {}\n",
    "   lem_corpus = []\n",
    "   flat_lem_corpus = []\n",
    "\n",
    "   lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "   for transcriptid, text in bigram_dict.items():\n",
    "      p_text = []\n",
    "      flat_list = []\n",
    "      for sentence in text:\n",
    "         lemmatized_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in sentence if word not in stop_words]\n",
    "         # print(f'ls: {lemmatized_sentence}')\n",
    "         p_text.append(lemmatized_sentence)\n",
    "         flat_list.extend(lemmatized_sentence)\n",
    "         lem_corpus.append(lemmatized_sentence)\n",
    "         flat_lem_corpus.extend(lemmatized_sentence)\n",
    "\n",
    "      num_tokens_after += len(flat_list)\n",
    "      lem_dict[transcriptid] = p_text\n",
    "      flat_lem_dict[transcriptid] = flat_list\n",
    "\n",
    "   return lem_dict, flat_lem_dict, lem_corpus, flat_lem_corpus\n",
    "\n",
    "\n",
    "# def process_the_data_lemmatizer(unprocessed_dict, stop_words):\n",
    "#    num_tokens_before = 0\n",
    "#    num_tokens_after = 0\n",
    "#    processed_dict = {}\n",
    "\n",
    "#    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#    for transcriptid, text in unprocessed_dict.items():\n",
    "#       p_text = []\n",
    "#       for sentence in text:\n",
    "#          p_sentence = []\n",
    "#          for word in sentence:\n",
    "#             num_tokens_before += 1\n",
    "#             p_word = word.lower()                                  #lowercase the text\n",
    "#             p_word = re.sub(r'(?<!\\w)-(?!\\w)|[^\\w\\s-]', '', p_word)\n",
    "#             p_sentence.append(p_word)  #remove punctuation but keep hyphens\n",
    "\n",
    "#          lemmatized_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "#                                  for word in p_sentence if word not in stop_words and len(word) > 0]\n",
    "\n",
    "#          p_text.append(lemmatized_sentence)\n",
    "#          num_tokens_after += len(lemmatized_sentence)\n",
    "#       processed_dict[transcriptid] = p_text\n",
    "#    return processed_dict\n",
    "\n",
    "\n",
    "# #below does not use lemmatizer. much faster.\n",
    "# def process_the_data(unprocessed_dict, stop_words):\n",
    "#    num_tokens_before = 0\n",
    "#    num_tokens_after = 0\n",
    "#    processed_dict = {}\n",
    "\n",
    "#    for transcriptid, text in unprocessed_dict.items():\n",
    "#       p_text = []\n",
    "#       for sentence in text:\n",
    "#          p_sentence = []\n",
    "#          for word in sentence:\n",
    "#             num_tokens_before += 1\n",
    "#             p_word = word.lower()                                  #lowercase the text\n",
    "#             p_word = re.sub(r'(?<!\\w)-(?!\\w)|[^\\w\\s-]', '', p_word)  #remove punctuation but keep hyphens\n",
    "#             if p_word in stop_words or len(p_word) == 0:             #ignore if word has no length (ie was punctuation only) or in stop words\n",
    "#                continue\n",
    "#             p_sentence.append(p_word)\n",
    "#          p_text.append(p_sentence)\n",
    "#          num_tokens_after += len(p_sentence)\n",
    "#       processed_dict[transcriptid] = p_text\n",
    "#    print(f'tokens before:{num_tokens_before} | tokens after:{num_tokens_after}')\n",
    "#    return processed_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# below function returns:\n",
    "#     lem_corpus: list of sentences for full corpus to run through word2vec\n",
    "#     dict_bigrams: links transcript id to corpus for uncertainty calc\n",
    "#     flat_lem_corpus: flat version of corpus bigrams for counting word appearances easily\n",
    "# \"\"\"\n",
    "\n",
    "# def create_bigrams(processed_dict, min_count=10, threshold = 100):\n",
    "#    processed_corpus = []\n",
    "#    for value in processed_dict.values():\n",
    "#       processed_corpus.extend(value)\n",
    "\n",
    "#    phrases = Phrases(processed_corpus, min_count, threshold, scoring='default')\n",
    "#    bigram_phraser = Phraser(phrases)\n",
    "\n",
    "#    lem_corpus = []\n",
    "\n",
    "#    dict_bigrams = {}\n",
    "#    for transcript_id, text in processed_dict.items():\n",
    "#       bigram_text = [bigram_phraser[sentence] for sentence in text]\n",
    "#       dict_bigrams[transcript_id] = bigram_text\n",
    "#       lem_corpus.extend(bigram_text)\n",
    "\n",
    "\n",
    "#    flat_lem_corpus = [item for sublist in lem_corpus for item in sublist]  #flatten list\n",
    "\n",
    "#    # print('bigrams created')\n",
    "#    return phrases, bigram_phraser, lem_corpus, dict_bigrams, flat_lem_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IDF dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create idf dictionary upon node creation by looking at uncertainty word list\n",
    "def create_idf_dict(node):\n",
    "    idf_dict = {}\n",
    "    n = len(node.lem_dict) #number of transcripts\n",
    "    for word in node.uncertainty_wordlist:\n",
    "        t = 0\n",
    "        for transcript_id, transcript_text in node.flat_lem_dict.items():\n",
    "            if word in transcript_text:\n",
    "                t+=1   #count if word appears in transcript\n",
    "\n",
    "        idf= math.log2(n/t)\n",
    "        idf_dict[word] = idf\n",
    "    return idf_dict\n",
    "\n",
    "\n",
    "#seperate run to create an idf dict for soto uncertainty words\n",
    "def create_idf_dict_soto_wordlist(node, soto_uncertainty_words):\n",
    "    idf_dict = {}\n",
    "    n = len(node.lem_dict)\n",
    "    for word in soto_uncertainty_words:\n",
    "        t = 1          #initialize at 1 so you don't divide by 0 if word never appears\n",
    "        for transcript_id, transcript_text in node.flat_lem_dict.items():\n",
    "            if word in transcript_text:\n",
    "                t+=1\n",
    "\n",
    "        idf= math.log2(n/t)\n",
    "        idf_dict[word] = idf\n",
    "\n",
    "    return idf_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build uncertainty words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses a list of seed words and collects top n words by proximity score\n",
    "\n",
    "def build_uncertainty_words(node, trained_model, seed_words, delta_df, return_number = 100):\n",
    "    result_dictionary = {}\n",
    "    for seed_word in seed_words:\n",
    "        try:\n",
    "            similar_words = trained_model.wv.most_similar(seed_word, topn=300)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for word, number in similar_words:\n",
    "            if word not in result_dictionary:\n",
    "                result_dictionary[word] = number\n",
    "            else:\n",
    "                result_dictionary[word] = max(number, result_dictionary[word])\n",
    "\n",
    "    ordered_words = sorted(result_dictionary, key=result_dictionary.get, reverse=True)     # orders word list by score\n",
    "    final_list = []                                                                        # checks that the word appears in at least two different companies\n",
    "    while len(final_list) < return_number:\n",
    "        for word in ordered_words:\n",
    "            ciq_id_count = transcript_lookup(word, node.flat_lem_dict, delta_df)\n",
    "            if ciq_id_count < 2:\n",
    "                continue\n",
    "            else: final_list.append(word)\n",
    "            if len(final_list) == return_number:\n",
    "                break\n",
    "\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Uncertainty scores - regular and soto variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculates uncertainty in a node by counting uncertainty words and dividing by bigram transcript length. Adds score to dataframe.\n",
    "def calc_uncertainty_reg(node, delta_df, column_name):\n",
    "    for transcript_id, transcript_text in node.flat_lem_dict.items():\n",
    "        uncty_cnt = 0\n",
    "        for word in transcript_text:\n",
    "            if word in node.uncertainty_wordlist:\n",
    "                uncty_cnt += 1\n",
    "        uncty_score = uncty_cnt/len(transcript_text)\n",
    "        try:\n",
    "            delta_df.loc[delta_df['transcript_id'] == transcript_id, column_name] = uncty_score\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "#calculates uncertainty in a node by counting uncertainty words if they appear, weighing by tf_idf, and dividing by number of words in transcript\n",
    "def calc_uncertainty_soto(node, delta_df, column_name):\n",
    "    for transcript_id, transcript_text in node.flat_lem_dict.items():\n",
    "        word_count_dict = Counter(transcript_text)    #creates a count dictionary for each word in transcript\n",
    "        d = len(transcript_text)\n",
    "        tf_idf_sum = 0\n",
    "        for word in node.uncertainty_wordlist:\n",
    "            if word in transcript_text:\n",
    "                t = word_count_dict[word]\n",
    "                tf = t/d\n",
    "                idf = node.idf_dict[word]\n",
    "                # print(f'word:{word}| t:{t}|d:{d}|tf:{tf}|idf:{idf}')\n",
    "                tf_idf = tf*idf\n",
    "                tf_idf_sum += tf_idf\n",
    "\n",
    "        uncty_score = tf_idf_sum/len(set(transcript_text))\n",
    "        # print(f'tf_idf_sum: {tf_idf_sum}| transcript length: {len(set(transcript_text))}|uncty_score:{uncty_score}')\n",
    "\n",
    "        try:\n",
    "            delta_df.loc[delta_df['transcript_id'] == transcript_id, column_name] = uncty_score #some transcripts missing if there wasn't any financial data\n",
    "            # print(uncty_score)\n",
    "        except KeyError:\n",
    "            # print(f'{transcript_id}: key error')\n",
    "            continue\n",
    "\n",
    "#calculates soto uncertainty using soto method\n",
    "def calc_uncertainty_soto_wordlist(node, delta_df, column_name, soto_uncertainty_words):\n",
    "    for transcript_id, transcript_text in node.flat_lem_dict.items():\n",
    "        word_count_dict = Counter(transcript_text)\n",
    "        d = len(transcript_text)\n",
    "        tf_idf_sum = 0\n",
    "        for word in soto_uncertainty_words:\n",
    "            if word in transcript_text:\n",
    "                t = word_count_dict[word]\n",
    "                tf = t/d\n",
    "                idf = node.idf_dict_full[word]\n",
    "                # print(f'word:{word}| t:{t}|d:{d}|tf:{tf}|idf:{idf}')\n",
    "                tf_idf = tf*idf\n",
    "                tf_idf_sum += tf_idf\n",
    "\n",
    "        uncty_score = tf_idf_sum/len(set(transcript_text))\n",
    "        # print(f'tf_idf_sum: {tf_idf_sum}| transcript length: {len(set(transcript_text))}|uncty_score:{uncty_score}')\n",
    "\n",
    "        try:\n",
    "            delta_df.loc[delta_df['transcript_id'] == transcript_id, column_name] = uncty_score\n",
    "            # print(uncty_score)\n",
    "        except KeyError:\n",
    "            # print(f'{transcript_id}: key error')\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend yearly corpus to full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functuon that takes each year and combines dictionaries to create one complete node\n",
    "def extend_to_full(model_node_dict):\n",
    "    lem_corpus_allyrs = []\n",
    "    flat_lem_corpus_allyrs = []\n",
    "    lem_dict_allyrs = {}\n",
    "    flat_lem_dict_allyrs = {}\n",
    "\n",
    "    for year, node in model_node_dict.items():\n",
    "        lem_corpus_allyrs.extend(node.lem_corpus)\n",
    "        flat_lem_corpus_allyrs.extend(node.flat_lem_corpus)\n",
    "        lem_dict_allyrs.update(node.lem_dict)\n",
    "        flat_lem_dict_allyrs.update(node.flat_lem_dict)\n",
    "\n",
    "    return lem_corpus_allyrs,flat_lem_corpus_allyrs,lem_dict_allyrs, flat_lem_dict_allyrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup word and year to see which which transcripts contain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_quarter_lookup(transcript_id, delta_df):\n",
    "\n",
    "    try:\n",
    "        ciq_id = delta_df.loc[delta_df['transcript_id'] == transcript_id, 'ciq_id'].values[0]\n",
    "        quarter = delta_df.loc[delta_df['transcript_id'] == transcript_id, 'quarter'].values[0]\n",
    "    except IndexError:\n",
    "        ciq_id = 11111\n",
    "        quarter = 'NA'\n",
    "    return(ciq_id,quarter)\n",
    "\n",
    "\n",
    "def transcript_lookup(word, dict, delta_df):\n",
    "    count_set = set()\n",
    "    for transcript_id, flat_list in dict.items():\n",
    "        if word in flat_list:\n",
    "            # print(word, transcript_id)\n",
    "            ciq_id, quarter = company_quarter_lookup(transcript_id, delta_df)\n",
    "            count_set.add(ciq_id)\n",
    "        if len(count_set) == 2:\n",
    "            break\n",
    "    return len(count_set)\n",
    "\n",
    "\n",
    "def transcript_lookup_print(word, dict, delta_df):\n",
    "    for transcript_id, flat_list in dict.items():\n",
    "        count_set = set()\n",
    "        if word in flat_list:\n",
    "            ciq_id, quarter = company_quarter_lookup(transcript_id, delta_df)\n",
    "            print(transcript_id, ciq_id, quarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_word2vec(lem_corpus, v_size=300, w = 10, mc = 10, e = 10):\n",
    "   model = gensim.models.Word2Vec (\n",
    "    vector_size=v_size,    # Number of features in word vector\n",
    "\n",
    "    window=w,   # Context window size (in each direction). Default is 5\n",
    "\n",
    "\n",
    "    min_count=mc, # Words must appear this many times to be in vocab.\n",
    "                 #   Default is 5\n",
    "\n",
    "    workers=10,  # Training thread count\n",
    "\n",
    "    sg=1,        # 0: CBOW, 1: Skip-gram.\n",
    "\n",
    "    hs=0,        # 0: Negative Sampling, 1: Hierarchical Softmax\n",
    "                 #   Default is 0, NS\n",
    "\n",
    "    negative=5   # Nmber of negative samples\n",
    "                 #   Default is 5\n",
    "   )\n",
    "\n",
    "   model.build_vocab(\n",
    "    lem_corpus\n",
    "   )\n",
    "\n",
    "   model.train(\n",
    "    lem_corpus,\n",
    "    total_examples=len(lem_corpus),\n",
    "    epochs=e        # How many training passes to take.\n",
    "   )\n",
    "\n",
    "   return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for processing years: 1111 seconds\n",
      "\n",
      "Time taken for extending years: 6 seconds\n",
      "\n",
      "Starting tokens: 66087551\n",
      "Tokens after cleaning: 673528828\n",
      "Tokens after bigrams: 57728275\n",
      "Tokens after full processing: 28929243\n"
     ]
    }
   ],
   "source": [
    "#Process Data\n",
    "\n",
    "num_tokens_b = 0\n",
    "num_tokens_a = 0\n",
    "num_tokens_ab = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for index, file in enumerate(csv_files):\n",
    "    tokenized_dict = get_corpus(file)\n",
    "    punct_removed_dict, num_tokens_before, num_tokens_after = clean(tokenized_dict)\n",
    "    phrases, bigram_phraser, bigram_dict, num_tokens_after_bigrams = create_bigrams_pre_lem(punct_removed_dict, min_count=10, threshold = 100)\n",
    "    lem_dict, flat_lem_dict, lem_corpus, flat_lem_corpus =  process_the_data_lemmatizer(bigram_dict, stop_words)\n",
    "    Nodes(years_list[index], lem_dict, flat_lem_dict, lem_corpus, flat_lem_corpus)\n",
    "\n",
    "    num_tokens_b += num_tokens_before\n",
    "    num_tokens_a += num_tokens_after\n",
    "    num_tokens_ab += num_tokens_after_bigrams\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for processing years: {end_time - start_time:.0f} seconds\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "lem_corpus_allyrs,flat_lem_corpus_allyrs,lem_dict_allyrs, flat_lem_dict_allyrs = extend_to_full(Nodes.node_dict)\n",
    "full_node = Combined_Node(lem_corpus=lem_corpus_allyrs, flat_lem_dict = flat_lem_dict_allyrs, lem_dict = lem_dict_allyrs, flat_lem_corpus = flat_lem_corpus_allyrs)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for extending years: {end_time - start_time:.0f} seconds\")\n",
    "\n",
    "print()\n",
    "print(f'Starting tokens: {num_tokens_b}')\n",
    "print(f'Tokens after cleaning: {num_tokens_a}')\n",
    "print(f'Tokens after bigrams: {num_tokens_ab}')\n",
    "print(f'Tokens after full processing: {len(flat_lem_corpus_allyrs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Models\n",
    "\n",
    "#yearly corupus\n",
    "model_time = 0\n",
    "wl_time = 0\n",
    "idf_time = 0\n",
    "\n",
    "for year, node in Nodes.node_dict.items():\n",
    "\n",
    "    start_time\n",
    "    trained_model = apply_word2vec(node.lem_corpus, e = 20)\n",
    "    end_time = time.time()\n",
    "    model_time += end_time - start_time\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    word_list = build_uncertainty_words( node,trained_model, seed_words, delta_df, 100)\n",
    "    end_time = time.time()\n",
    "    wl_time += end_time - start_time\n",
    "\n",
    "\n",
    "    node.model=trained_model\n",
    "    node.uncertainty_wordlist=word_list\n",
    "\n",
    "    start_time = time.time()\n",
    "    idf_dict = create_idf_dict(node)\n",
    "    end_time = time.time()\n",
    "    idf_time += end_time - start_time\n",
    "\n",
    "    node.idf_dict = idf_dict\n",
    "\n",
    "print(f\"Time taken for model creation: {model_time} seconds\")\n",
    "print(f\"Time taken for getting word lists: {wl_time} seconds\")\n",
    "print(f\"Time taken for creating idf: {idf_time} seconds\")\n",
    "\n",
    "#full corpus\n",
    "# start_time = time.time()\n",
    "# trained_model = apply_word2vec(full_node.lem_corpus)\n",
    "# end_time = time.time()\n",
    "# print(f\"Time taken for full node model creation: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# word_list = build_uncertainty_words(full_node, trained_model, seed_words, delta_df, 150)\n",
    "\n",
    "\n",
    "# full_node.model=trained_model\n",
    "# full_node.uncertainty_wordlist=word_list\n",
    "# start_time = time.time()\n",
    "# idf_dict = create_idf_dict(full_node)\n",
    "# end_time = time.time()\n",
    "# print(f\"Time taken for full node idf creation {year}: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# full_node.idf_dict = idf_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trained_model = apply_word2vec(full_node.lem_corpus, e=20)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for full node model creation: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "word_list = build_uncertainty_words(full_node, trained_model, seed_words, delta_df, 150)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for to build uncertainty word list: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "full_node.model=trained_model\n",
    "full_node.uncertainty_wordlist=word_list\n",
    "start_time = time.time()\n",
    "idf_dict = create_idf_dict(full_node)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for full node idf creation {year}: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "full_node.idf_dict = idf_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Bigrams and Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count = 0\n",
    "unique_bigrams = set()\n",
    "unique_tokens = set()\n",
    "\n",
    "# Loop through the list and check for underscores\n",
    "for token in full_node.flat_lem_corpus:\n",
    "    unique_tokens.add(token)\n",
    "    if \"_\" in token:\n",
    "        bigram_count += 1\n",
    "        unique_bigrams.add(token)\n",
    "\n",
    "print(f'Total Tokens: {len(full_node.flat_lem_corpus)}')\n",
    "print(f'Unique Tokens: {len(unique_tokens)}')\n",
    "print(f'Total Bigrams: {bigram_count}')\n",
    "print(f'Unique Bigrams: {len(unique_bigrams)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# idf_dict_full = create_idf_dict_soto_wordlist(full_node, soto_uncertainty_words)\n",
    "# full_node.idf_dict_full = idf_dict_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Uncertainty Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreates idf_dicts with up-to-date uncertainty wordlists\n",
    "\n",
    "# for year, node in Nodes.node_dict.items():\n",
    "#     idf_dict = create_idf_dict(node)\n",
    "#     node.idf_dict = idf_dict\n",
    "\n",
    "# full_node.idf_dict = create_idf_dict(full_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate all the different uncertainty scores and add to dataframe\n",
    "\n",
    "for year, node in Nodes.node_dict.items():\n",
    "    calc_uncertainty_reg(node, delta_df, 'yearnode_uncty_reg')\n",
    "\n",
    "\n",
    "calc_uncertainty_reg(full_node, delta_df, 'fullnode_uncty_reg')\n",
    "\n",
    "for year, node in Nodes.node_dict.items():\n",
    "    calc_uncertainty_soto(node, delta_df, 'yearnode_uncty_soto')\n",
    "\n",
    "\n",
    "calc_uncertainty_soto(full_node, delta_df, 'fullnode_uncty_soto')\n",
    "\n",
    "print(tabulate(delta_df.head(), headers='keys', tablefmt='pretty'))\n",
    "\n",
    "# calc_uncertainty_soto_wordlist(full_node, delta_df, 'fullnode_uncty_soto_wl', soto_uncertainty_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Word Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_uncertainty_wordlist = []   #creates a list of all uncertainty words by year\n",
    "\n",
    "for year in years_list:\n",
    "    for uncertainty_word in Nodes.node_dict[year].uncertainty_wordlist:\n",
    "        if uncertainty_word not in combined_uncertainty_wordlist:\n",
    "            combined_uncertainty_wordlist.append(uncertainty_word)\n",
    "\n",
    "print(len(combined_uncertainty_wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count total instances of word\n",
    "word_freq_df = pd.DataFrame(index=years_list, columns=combined_uncertainty_wordlist)\n",
    "\n",
    "for year in years_list:\n",
    "    word_counts = Counter((Nodes.node_dict[year].flat_lem_corpus))\n",
    "    for uncertainty_word in combined_uncertainty_wordlist:\n",
    "        count = word_counts[uncertainty_word]\n",
    "        word_freq_df.at[year, uncertainty_word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years_list:\n",
    "    print(year)\n",
    "    count = 0\n",
    "\n",
    "    for word in Nodes.node_dict[year].uncertainty_wordlist:\n",
    "        average_count = word_freq_df[word].mean()\n",
    "        year_count = word_freq_df.at[year, word]\n",
    "\n",
    "        if year_count/average_count > 3:\n",
    "            count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yearly Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame()\n",
    "\n",
    "for year in years_list:\n",
    "    # print(year)\n",
    "    word_series = []\n",
    "    for word in Nodes.node_dict[year].uncertainty_wordlist:\n",
    "        average_count = word_freq_df[word].mean()\n",
    "        year_count = word_freq_df.at[year, word]\n",
    "        # if year_count/average_count > 0:\n",
    "        word_series.append((year_count/average_count,word))\n",
    "        sorted_pairs = sorted(word_series, reverse=True)\n",
    "        sorted_values, sorted_words = zip(*sorted_pairs)\n",
    "        sorted_words = list(sorted_words)\n",
    "\n",
    "    export_df[year] = pd.Series(sorted_words)\n",
    "\n",
    "            # print(f\"{word} | {year_count}\")\n",
    "\n",
    "export_df.fillna('', inplace=True)\n",
    "print(tabulate(export_df, headers='keys', tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_df.to_csv('yearly_word_run4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcript Look Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_lookup_print('fas_91', Nodes.node_dict[2014].flat_lem_dict, delta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in Nodes.node_dict[2014].flat_lem_dict[568757]:\n",
    "    if word in Nodes.node_dict[2014].uncertainty_wordlist:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    name = delta_df.loc[delta_df['transcript_id'] == 568757, 'l2a_delta'].iloc[0]\n",
    "    print(f\"The name for primary_key {key} is {name}\")\n",
    "except IndexError:\n",
    "    print(f\"Primary key {key} not found in the DataFrame.\")\n",
    "\n",
    "name = delta_df.loc[delta_df['transcript_id'] == 2925824, 'l2a_delta'].iloc[0]\n",
    "print(f\"The name for primary_key {key} is {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Corpus Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(full_node.flat_lem_corpus)\n",
    "for uncertainty_word in full_node.uncertainty_wordlist:\n",
    "        count = word_counts[uncertainty_word]\n",
    "        print(f\"{uncertainty_word} | {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare soto list vs my uncertainty list\n",
    "\n",
    "\n",
    "\n",
    "common_words = set(soto_uncertainty_words) & set(full_node.uncertainty_wordlist)\n",
    "for word in common_words:\n",
    "    print(word)\n",
    "print('-----')\n",
    "print('Just in soto')\n",
    "print('-----')\n",
    "difference_list1 = list(set(soto_uncertainty_words) - set(full_node.uncertainty_wordlist))\n",
    "for word in difference_list1:\n",
    "    print(word)\n",
    "print('-----')\n",
    "print('Just in mine')\n",
    "print('-----')\n",
    "difference_list2 = list(set(full_node.uncertainty_wordlist) - set(soto_uncertainty_words))\n",
    "for word in difference_list2:\n",
    "    print(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of uncertainty score vs loan movement\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "mean_value = delta_df['yearnode_uncty_reg'].mean()\n",
    "correlation = delta_df['yearnode_uncty_reg'].corr(delta_df['l2a_delta'])\n",
    "axs[0,0].scatter( delta_df['yearnode_uncty_reg'],delta_df['l2a_delta'])\n",
    "axs[0,0].set_xlabel('Uncertainty Score')\n",
    "axs[0,0].set_ylabel('Loan Movement')\n",
    "axs[0,0].set_title('Yearly Splits with Standard Uncertainty Calc.')\n",
    "axs[0,0].text(0.5, -0.2, 'Corerelation Coefficient: '+str(round(correlation, 4)), ha='center', transform=axs[0, 0].transAxes)\n",
    "axs[0,0].axvline(mean_value, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_value:.2f}')\n",
    "\n",
    "mean_value = delta_df['yearnode_uncty_soto'].mean()\n",
    "correlation = delta_df['yearnode_uncty_soto'].corr(delta_df['l2a_delta'])\n",
    "axs[0,1].scatter( delta_df['yearnode_uncty_soto'],delta_df['l2a_delta'])\n",
    "axs[0,1].set_xlabel('Uncertainty Score')\n",
    "axs[0,1].set_ylabel('Loan Movement')\n",
    "axs[0,1].set_title('Yearly Splits with TF_IDF Weighting.')\n",
    "axs[0,1].ticklabel_format(style='scientific', axis='x', scilimits=(0, 0))\n",
    "axs[0,1].text(0.5, -0.2, 'Corerelation Coefficient: '+str(round(correlation, 4)), ha='center', transform=axs[0, 1].transAxes)\n",
    "axs[0,1].axvline(mean_value, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_value:.2f}')\n",
    "\n",
    "mean_value = delta_df['fullnode_uncty_reg'].mean()\n",
    "correlation = delta_df['fullnode_uncty_reg'].corr(delta_df['l2a_delta'])\n",
    "axs[1,0].scatter( delta_df['fullnode_uncty_reg'],delta_df['l2a_delta'])\n",
    "axs[1,0].set_xlabel('Uncertainty Score')\n",
    "axs[1,0].set_ylabel('Loan Movement')\n",
    "axs[1,0].set_title('Full Data with Standard Uncertainty Calc.')\n",
    "axs[1,0].text(0.5, -0.2, 'Corerelation Coefficient: '+str(round(correlation, 4)), ha='center', transform=axs[1, 0].transAxes)\n",
    "axs[1,0].axvline(mean_value, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_value:.2f}')\n",
    "\n",
    "mean_value = delta_df['fullnode_uncty_soto'].mean()\n",
    "correlation = delta_df['fullnode_uncty_soto'].corr(delta_df['l2a_delta'])\n",
    "axs[1,1].scatter( delta_df['fullnode_uncty_soto'],delta_df['l2a_delta'])\n",
    "axs[1,1].set_xlabel('Uncertainty Score')\n",
    "axs[1,1].set_ylabel('Loan Movement')\n",
    "axs[1,1].set_title('Full Data with TF_IDF Weighting.')\n",
    "axs[1,1].ticklabel_format(style='scientific', axis='x', scilimits=(0, 0))\n",
    "axs[1,1].text(0.5, -0.2, 'Corerelation Coefficient: '+str(round(correlation, 4)), ha='center', transform=axs[1, 1].transAxes)\n",
    "axs[1,1].axvline(mean_value, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_value:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_value = delta_df['fullnode_uncty_soto_wl'].mean()\n",
    "# correlation = delta_df['fullnode_uncty_soto_wl'].corr(delta_df['l2a_delta'])\n",
    "# plt.scatter( delta_df['fullnode_uncty_soto_wl'],delta_df['l2a_delta'])\n",
    "# plt.xlabel('Uncertainty Score')\n",
    "# plt.ylabel('Loan Movement')\n",
    "# plt.title('Soto Word List with TF-IDF Weighting')\n",
    "# plt.text(0.5, -0.2, 'Corerelation Coefficient: '+str(round(correlation, 4)), ha='center', transform=plt.gca().transAxes)\n",
    "# plt.axvline(mean_value, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_value:.2f}')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_count = (delta_df['loan_diff'] < 0).sum()\n",
    "total = len(delta_df)\n",
    "\n",
    "print(negative_count)\n",
    "print(negative_count/total)\n",
    "print(len(delta_df))\n",
    "print((delta_df['l2a_delta'] < 0).sum())\n",
    "print((delta_df['l2a_delta']).mean())\n",
    "print((delta_df['l2a_delta']).var())\n",
    "print(delta_df['l2a_delta'].abs().var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_rows = delta_df.nlargest(10, 'yearnode_uncty_soto')\n",
    "print(tabulate(top_10_rows.sort_values(by='yearnode_uncty_soto', ascending=False), headers='keys', tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (v_size=300, w = 10, mc = 10, e = 10)\n",
    "\n",
    "hyperparameters = [(150, 10, 10, 10), (300, 10, 10, 10), (400, 10, 10, 10), (500, 10, 10, 10),\n",
    "    (300, 5, 10, 10), (300, 10, 10, 10), (300, 15, 10, 10), (300, 20, 10, 10),\n",
    "    (300, 10, 5, 10), (300, 10, 10, 10), (300, 10, 15, 10), (300, 10, 20, 10),\n",
    "    (300, 10, 10, 5), (300, 10, 10, 10), (300, 10, 10, 15), (300, 10, 10, 20)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_run(node_dict, hyperparameters, delta_df, seed_words):\n",
    "    tracker_dict = {}\n",
    "\n",
    "    for param_set in hyperparameters:\n",
    "        feature_size = param_set[0]\n",
    "        context_window = param_set[1]\n",
    "        min_count = param_set[2]\n",
    "        epochs = param_set[3]\n",
    "\n",
    "        for year, node in node_dict.items():\n",
    "            trained_model = apply_word2vec(node.lem_corpus, v_size=feature_size, w = context_window, mc = min_count, e = epochs)\n",
    "            word_list = build_uncertainty_words( node,trained_model, seed_words, delta_df, 100)\n",
    "\n",
    "            node.model=trained_model\n",
    "            node.uncertainty_wordlist=word_list\n",
    "\n",
    "            idf_dict = create_idf_dict(node)\n",
    "            node.idf_dict = idf_dict\n",
    "\n",
    "        trained_model = apply_word2vec(full_node.lem_corpus, v_size=feature_size, w = context_window, mc = min_count, e = epochs)\n",
    "        word_list = build_uncertainty_words(full_node, trained_model, seed_words, delta_df, 150)\n",
    "\n",
    "        full_node.model=trained_model\n",
    "        full_node.uncertainty_wordlist=word_list\n",
    "        idf_dict = create_idf_dict(full_node)\n",
    "        full_node.idf_dict = idf_dict\n",
    "\n",
    "        for year, node in node_dict.items():\n",
    "            calc_uncertainty_reg(node, delta_df, 'yearnode_uncty_reg')\n",
    "        calc_uncertainty_reg(full_node, delta_df, 'fullnode_uncty_reg')\n",
    "        for year, node in node_dict.items():\n",
    "            calc_uncertainty_soto(node, delta_df, 'yearnode_uncty_soto')\n",
    "        calc_uncertainty_soto(full_node, delta_df, 'fullnode_uncty_soto')\n",
    "\n",
    "        yearnode_uncty_reg = delta_df['yearnode_uncty_reg'].corr(delta_df['l2a_delta'])\n",
    "        fullnode_uncty_reg = delta_df['fullnode_uncty_reg'].corr(delta_df['l2a_delta'])\n",
    "        yearnode_uncty_soto = delta_df['yearnode_uncty_soto'].corr(delta_df['l2a_delta'])\n",
    "        fullnode_uncty_soto = delta_df['fullnode_uncty_soto'].corr(delta_df['l2a_delta'])\n",
    "\n",
    "        combined_yearly_uncertainty_wordlist = set()\n",
    "        for key in node_dict.keys():\n",
    "            for word in node_dict[key].uncertainty_wordlist:\n",
    "                combined_yearly_uncertainty_wordlist.add(word)\n",
    "        unique_word_count = len(combined_yearly_uncertainty_wordlist)\n",
    "\n",
    "\n",
    "        word_freq_df = pd.DataFrame(index=list(node_dict.keys()), columns=list(combined_yearly_uncertainty_wordlist))\n",
    "\n",
    "        for key in node_dict.keys():\n",
    "            word_counts = Counter(node_dict[key].flat_lem_corpus)\n",
    "            for uncertainty_word in combined_yearly_uncertainty_wordlist:\n",
    "                count = word_counts[uncertainty_word]\n",
    "                word_freq_df.at[key, uncertainty_word] = count\n",
    "\n",
    "\n",
    "        rare_words = []\n",
    "        for key in node_dict.keys():\n",
    "            for word in node_dict[key].uncertainty_wordlist:\n",
    "                average_count = word_freq_df[word].mean()\n",
    "                year_count = word_freq_df.at[key, word]\n",
    "                if year_count/average_count > 3:\n",
    "                    rare_words.append(word)\n",
    "\n",
    "        average_rare_words = len(rare_words)/len(node_dict)\n",
    "\n",
    "        common_words_count = len(set(soto_uncertainty_words) & set(full_node.uncertainty_wordlist))\n",
    "\n",
    "        correlation_calcs = [yearnode_uncty_reg, fullnode_uncty_reg, yearnode_uncty_soto, fullnode_uncty_soto]\n",
    "\n",
    "        tracker_dict[param_set]= [unique_word_count, average_rare_words, common_words_count, correlation_calcs,full_node.uncertainty_wordlist]\n",
    "\n",
    "    return tracker_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_dict = repeat_run(Nodes.node_dict, hyperparameters, delta_df, seed_words)\n",
    "\n",
    "for key, value in tracker_dict.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in tracker_dict.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Run - single hyperparameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_run(node_dict, delta_df, seed_words, loop_count = 10):\n",
    "    tracker_dict = {}\n",
    "    for key in delta_df['transcript_id']:\n",
    "        try:\n",
    "            tracker_dict[key] = []\n",
    "        except:\n",
    "            continue\n",
    "    wordlist_dict = {}\n",
    "    correlation_dict = {}\n",
    "    for i in range(loop_count):\n",
    "        np.random.seed(i)\n",
    "\n",
    "        feature_size = 300\n",
    "        context_window = 10\n",
    "        min_count = 10\n",
    "        epochs = 20\n",
    "\n",
    "        for year, node in node_dict.items():\n",
    "            trained_model = apply_word2vec(node.lem_corpus, v_size=feature_size, w = context_window, mc = min_count, e = epochs)\n",
    "            word_list = build_uncertainty_words( node,trained_model, seed_words, delta_df, 100)\n",
    "\n",
    "            node.model=trained_model\n",
    "            node.uncertainty_wordlist=word_list\n",
    "\n",
    "            idf_dict = create_idf_dict(node)\n",
    "            node.idf_dict = idf_dict\n",
    "\n",
    "        trained_model = apply_word2vec(full_node.lem_corpus, v_size=feature_size, w = context_window, mc = min_count, e = epochs)\n",
    "        word_list = build_uncertainty_words(full_node, trained_model, seed_words, delta_df, 150)\n",
    "        wordlist_dict[i] = word_list\n",
    "\n",
    "        full_node.model=trained_model\n",
    "        full_node.uncertainty_wordlist=word_list\n",
    "        idf_dict = create_idf_dict(full_node)\n",
    "        full_node.idf_dict = idf_dict\n",
    "\n",
    "        for year, node in node_dict.items():\n",
    "            calc_uncertainty_reg(node, delta_df, 'yearnode_uncty_reg')\n",
    "        calc_uncertainty_reg(full_node, delta_df, 'fullnode_uncty_reg')\n",
    "        for year, node in node_dict.items():\n",
    "            calc_uncertainty_soto(node, delta_df, 'yearnode_uncty_soto')\n",
    "        calc_uncertainty_soto(full_node, delta_df, 'fullnode_uncty_soto')\n",
    "\n",
    "        yearnode_uncty_reg = delta_df['yearnode_uncty_reg'].corr(delta_df['l2a_delta'])\n",
    "        fullnode_uncty_reg = delta_df['fullnode_uncty_reg'].corr(delta_df['l2a_delta'])\n",
    "        yearnode_uncty_soto = delta_df['yearnode_uncty_soto'].corr(delta_df['l2a_delta'])\n",
    "        fullnode_uncty_soto = delta_df['fullnode_uncty_soto'].corr(delta_df['l2a_delta'])\n",
    "\n",
    "        correlation_dict[i] = [yearnode_uncty_reg, fullnode_uncty_reg, yearnode_uncty_soto, fullnode_uncty_soto]\n",
    "\n",
    "\n",
    "        for _, row in delta_df.iterrows():\n",
    "            try:\n",
    "                transcript_id = row['transcript_id']\n",
    "                value1 = row['yearnode_uncty_reg']\n",
    "                value2 = row['fullnode_uncty_reg']\n",
    "                value3 = row['yearnode_uncty_soto']\n",
    "                value4 = row['fullnode_uncty_soto']\n",
    "\n",
    "                tracker_dict[transcript_id].append((value1, value2, value3, value4))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "        print(f'loop: {i}')\n",
    "    return tracker_dict, wordlist_dict, correlation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_df, wordlist_dict, correlation_dict = loop_run(Nodes.node_dict, delta_df, seed_words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_dict = {}\n",
    "\n",
    "for key, value in wordlist_dict.items():\n",
    "    for word in value:\n",
    "        if word not in counting_dict:\n",
    "            counting_dict[word] = 1\n",
    "        else:\n",
    "            counting_dict[word] += 1\n",
    "\n",
    "print(f'There are {len(counting_dict)} words total.')\n",
    "\n",
    "sorted_word_count = sorted(counting_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, count in sorted_word_count:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(counting_dict.keys())[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_node.uncertainty_wordlist = list(counting_dict.keys())[:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_uncertainty_reg(full_node, delta_df, 'fullnode_uncty_reg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearnode_uncty_reg = []\n",
    "fullnode_uncty_reg = []\n",
    "yearnode_uncty_soto = []\n",
    "fullnode_uncty_soto = []\n",
    "\n",
    "\n",
    "for key, value in correlation_dict.items():\n",
    "    yearnode_uncty_reg.append(value[0])\n",
    "    fullnode_uncty_reg.append(value[1])\n",
    "    yearnode_uncty_soto.append(value[2])\n",
    "    fullnode_uncty_soto.append(value[3])\n",
    "\n",
    "\n",
    "print(yearnode_uncty_reg)\n",
    "print(fullnode_uncty_reg)\n",
    "print(yearnode_uncty_soto)\n",
    "print(fullnode_uncty_soto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('lemmatized_yearly_.pkl', 'wb') as f:\n",
    "    pickle.dump(Nodes.node_dict, f)\n",
    "\n",
    "with open('lemmatized_full_data.pkl', 'wb') as f:\n",
    "    pickle.dump(full_node, f)\n",
    "\n",
    "\n",
    "print(\"Dictionary saved'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df.to_csv('no_lemmatization_uncertainty_export_final2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_node.uncertainty_wordlist))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
