{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# nltk.download('punkt') # Download the 'punkt' tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.lower()      # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   # Remove punctuation and special characters\n",
    "    if text in stop_words:                    #remove stop-words based on nltk set\n",
    "        text = 'SKIP'\n",
    "    elif text == '':\n",
    "        text = 'SKIP'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Process Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens before processing: 1,161,192\n",
      "\n",
      "Number of Tokens after processing: 539,921\n",
      "Process Sentence Example:\n",
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n",
      "['jury', 'said', 'termend', 'presentments', 'city', 'executive', 'committee', 'overall', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted']\n",
      "['septemberoctober', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'hardfought', 'primary', 'mayornominate', 'ivan', 'allen', 'jr']\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents()   #list of sentences, each sentence is a list of words\n",
    "\n",
    "flattened_list = [word for sentence in sentences for word in sentence]\n",
    "print(f'Number of Tokens before processing: {len(flattened_list):,}')\n",
    "print()\n",
    "\n",
    "num_tokens = 0\n",
    "processed_sents = []\n",
    "for sent in sentences:\n",
    "   p_sentence = []\n",
    "   for word in sent:\n",
    "     word = preprocess_text(word)\n",
    "     if word != 'SKIP':\n",
    "        p_sentence.append(word)\n",
    "   processed_sents.append(p_sentence)\n",
    "   num_tokens += len(p_sentence)\n",
    "\n",
    "print(f'Number of Tokens after processing: {num_tokens:,}')\n",
    "print(f'Process Sentence Example:')\n",
    "for i in processed_sents[:3]:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['jury', 'said', 'termend', 'presentments', 'city', 'executive', 'committee', 'overall', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted']\n"
     ]
    }
   ],
   "source": [
    "#check processing\n",
    "print(sentences[1])\n",
    "print(processed_sents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec (\n",
    "    vector_size=100,    # Number of features in word vector\n",
    "\n",
    "    window=10,   # Context window size (in each direction). Default is 5\n",
    "\n",
    "\n",
    "    min_count=5, # Words must appear this many times to be in vocab.\n",
    "                 #   Default is 5\n",
    "\n",
    "    workers=10,  # Training thread count\n",
    "\n",
    "    sg=1,        # 0: CBOW, 1: Skip-gram.\n",
    "\n",
    "    hs=0,        # 0: Negative Sampling, 1: Hierarchical Softmax\n",
    "                 #   Default is 0, NS\n",
    "\n",
    "    negative=5   # Nmber of negative samples\n",
    "                 #   Default is 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(\n",
    "    processed_sents,\n",
    "    progress_per=20000  # Tweaks how often progress is reported\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "  Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training the model...')\n",
    "\n",
    "model.train(\n",
    "    processed_sents,\n",
    "    total_examples=len(processed_sents),\n",
    "    epochs=10,        # How many training passes to take.\n",
    "    report_delay=10.0 # Report progress every 10 seconds.\n",
    ")\n",
    "\n",
    "print('  Done.')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showcase sample word vector and results layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for Uncertain:\n",
      "Length of vector: 100\n",
      "[-0.3484722   0.10163841 -0.01252631  0.01786912  0.17679088 -0.30190015\n",
      " -0.26260594  0.10047907 -0.07748776  0.09296066 -0.18984024 -0.12112519\n",
      " -0.19701016 -0.22183388  0.2633721  -0.20558923  0.28330415 -0.16902506\n",
      "  0.04455652 -0.38614827 -0.02205837  0.12179832  0.18581161 -0.08096561\n",
      "  0.12242373  0.04912557 -0.11092206  0.22599871 -0.17253287  0.29855505\n",
      "  0.3332483   0.0661625  -0.0473337   0.00911532  0.06316879  0.06293564\n",
      "  0.02696742 -0.0372304  -0.06809297 -0.19113077 -0.05350975  0.00749517\n",
      " -0.03151321 -0.09120497  0.21621387 -0.13368987 -0.04868797  0.03340903\n",
      " -0.00315683  0.11653571  0.07077207 -0.05241055 -0.21221593  0.09143794\n",
      " -0.28332743  0.22272316  0.22338995  0.03178673 -0.30422604  0.28374264\n",
      " -0.00108746  0.07090506  0.11222957 -0.04180789 -0.2734059   0.31775865\n",
      "  0.15461881 -0.08890928 -0.04745176  0.23896833  0.05201704 -0.00560736\n",
      "  0.24608867 -0.13530016  0.19973059  0.2777233   0.09203767 -0.03666746\n",
      " -0.2879699  -0.02727102 -0.48390394  0.1249141   0.1051375   0.26319677\n",
      "  0.03179618 -0.21270823 -0.18668647  0.35604048  0.37810835  0.03003606\n",
      "  0.01858783  0.0976871  -0.18324868  0.0751633   0.2597924  -0.04570511\n",
      "  0.1606021   0.06243643 -0.04441595 -0.00790171]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "vector = word_vectors['uncertain']\n",
    "\n",
    "print('Vector for Uncertain:')\n",
    "print(f'Length of vector: {len(vector)}')\n",
    "print(f'{vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocab List: 14,046\n",
      "['much', 'way', 'people', 'mr', 'us', 'little', 'state', 'good', 'make', 'world', 'still', 'see', 'men', 'work', 'long', 'get', 'life', 'never', 'day', 'another']\n"
     ]
    }
   ],
   "source": [
    "vocab = model.wv.key_to_index.keys()\n",
    "vocab_list = []\n",
    "for i in vocab:\n",
    "    vocab_list.append(i)\n",
    "\n",
    "print(f'Length of Vocab List: {len(vocab_list):,}')\n",
    "print(vocab_list[20:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>much</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>way</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mr</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>little</td>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>state</td>\n",
       "      <td>807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>make</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>world</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>still</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>see</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>men</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>work</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>long</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>get</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>life</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>never</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>day</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>another</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Count\n",
       "0      much    937\n",
       "1       way    909\n",
       "2    people    847\n",
       "3        mr    844\n",
       "4        us    838\n",
       "5    little    831\n",
       "6     state    807\n",
       "7      good    806\n",
       "8      make    794\n",
       "9     world    787\n",
       "10    still    782\n",
       "11      see    772\n",
       "12      men    763\n",
       "13     work    762\n",
       "14     long    753\n",
       "15      get    749\n",
       "16     life    715\n",
       "17    never    697\n",
       "18      day    687\n",
       "19  another    684"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts = []\n",
    "\n",
    "for i in range(20,40):\n",
    "    # Pick a random word.\n",
    "    word = vocab_list[i]\n",
    "    count = model.wv.get_vecattr(word, 'count')\n",
    "    word_counts.append((word, count))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(word_counts, columns=['Word', 'Count'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positivist: Similarity = 0.8871\n",
      "authenticity: Similarity = 0.8681\n",
      "incorrect: Similarity = 0.8680\n",
      "urges: Similarity = 0.8644\n",
      "experimenter: Similarity = 0.8608\n",
      "catharsis: Similarity = 0.8605\n",
      "uniqueness: Similarity = 0.8587\n",
      "mediums: Similarity = 0.8556\n",
      "comprehend: Similarity = 0.8552\n",
      "gabriels: Similarity = 0.8537\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('uncertain', topn=10)\n",
    "\n",
    "# Print the most similar words and their similarity scores\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: Similarity = {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
