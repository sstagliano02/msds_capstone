{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# nltk.download('punkt') # Download the 'punkt' tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.lower()      # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', 'SKIP', text)   # Remove punctuation and special characters\n",
    "    if text in stop_words:\n",
    "        text = 'SKIP'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens before processing: 1,161,192\n",
      "\n",
      "Number of Tokens after processing: 687,794\n",
      "Process Sentence Example:\n",
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', '', 'evidence', '', 'irregularities', 'took', 'place', '']\n",
      "['jury', 'said', 'termend', 'presentments', 'city', 'executive', 'committee', '', 'overall', 'charge', 'election', '', '', 'deserves', 'praise', 'thanks', 'city', 'atlanta', '', 'manner', 'election', 'conducted', '']\n",
      "['septemberoctober', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', '', 'irregularities', '', 'hardfought', 'primary', 'mayornominate', 'ivan', 'allen', 'jr', '']\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents()   #list of sentences, each sentence is a list of words\n",
    "\n",
    "flattened_list = [word for sentence in sentences for word in sentence]\n",
    "print(f'Number of Tokens before processing: {len(flattened_list):,}')\n",
    "print()\n",
    "\n",
    "num_tokens = 0\n",
    "processed_sents = []\n",
    "for sent in sentences:\n",
    "   p_sentence = []\n",
    "   for word in sent:\n",
    "     word = preprocess_text(word)\n",
    "     if word != 'SKIP':\n",
    "        p_sentence.append(word)\n",
    "   processed_sents.append(p_sentence)\n",
    "   num_tokens += len(p_sentence)\n",
    "\n",
    "print(f'Number of Tokens after processing: {num_tokens:,}')\n",
    "print(f'Process Sentence Example:')\n",
    "for i in processed_sents[:3]:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['jury', 'said', 'termend', 'presentments', 'city', 'executive', 'committee', '', 'overall', 'charge', 'election', '', '', 'deserves', 'praise', 'thanks', 'city', 'atlanta', '', 'manner', 'election', 'conducted', '']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])\n",
    "print(processed_sents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec (\n",
    "    vector_size=100,    # Number of features in word vector\n",
    "\n",
    "    window=10,   # Context window size (in each direction). Default is 5\n",
    "\n",
    "\n",
    "    min_count=5, # Words must appear this many times to be in vocab.\n",
    "                 #   Default is 5\n",
    "\n",
    "    workers=10,  # Training thread count\n",
    "\n",
    "    sg=1,        # 0: CBOW, 1: Skip-gram.\n",
    "\n",
    "    hs=0,        # 0: Negative Sampling, 1: Hierarchical Softmax\n",
    "                 #   Default is 0, NS\n",
    "\n",
    "    negative=5   # Nmber of negative samples\n",
    "                 #   Default is 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(\n",
    "    processed_sents,\n",
    "    progress_per=20000  # Tweaks how often progress is reported\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showcase sample word vector and results layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for Uncertain:\n",
      "Length of vector: 100\n",
      "[-1.3875127e-03 -8.1217596e-03 -4.2979526e-03  5.8132838e-03\n",
      " -2.4545563e-03  5.9709321e-03 -7.2090640e-03  7.6677990e-03\n",
      "  9.2748285e-04  2.6469468e-04  2.2181988e-04  6.4045666e-03\n",
      " -4.3719206e-03  2.5470508e-03 -2.4282443e-03  9.7007211e-03\n",
      "  9.2004649e-03  1.6240835e-04  8.5140290e-03  7.6306891e-03\n",
      "  8.9184018e-03 -8.3793399e-05  6.6874959e-03 -6.7278384e-03\n",
      "  3.7678909e-03  8.1698978e-03  6.1578345e-03  4.4980170e-03\n",
      "  4.6713445e-03 -4.9447883e-03 -7.0395377e-03  1.1095536e-03\n",
      "  1.2271714e-03 -9.3590729e-03  6.8509304e-03 -8.3757816e-03\n",
      "  2.0340241e-03  4.6982528e-03  9.1343792e-03  5.8620260e-03\n",
      "  5.0437334e-04 -1.6658187e-04  3.5977876e-03  9.3553634e-03\n",
      "  6.6908384e-03  6.3942005e-03 -8.5568856e-03 -9.6110357e-03\n",
      " -8.0016017e-04  1.4892805e-03 -7.2910904e-04 -4.7898972e-03\n",
      "  3.7911772e-03  5.1792930e-03 -9.5618665e-03  1.2392449e-03\n",
      "  3.8865351e-03 -4.6941852e-03 -1.4235448e-03  2.7174198e-03\n",
      " -9.4392328e-03 -2.0232988e-03  1.8521762e-03 -6.4657605e-03\n",
      "  1.4303911e-03  9.1746775e-03  7.9349317e-03 -2.9850483e-03\n",
      " -9.1596423e-03 -8.9505864e-03 -9.1549102e-03 -3.6198474e-03\n",
      " -5.7932688e-03  2.6106453e-03  3.5362137e-03  5.2656936e-03\n",
      " -9.9080885e-03  8.9306710e-04 -6.9734693e-04 -5.5797459e-03\n",
      "  3.7854624e-03  6.2357662e-03 -9.6058119e-03  8.6456826e-03\n",
      "  6.8433844e-03 -5.1252521e-03  4.1367100e-03 -9.1351988e-04\n",
      " -8.1490623e-03  3.6079383e-03  8.1851175e-03 -4.8067640e-03\n",
      " -9.1544389e-05 -7.5566997e-03 -6.5982724e-03  3.3411479e-03\n",
      "  1.1699545e-03  7.4629020e-03 -5.9379889e-03 -1.7588508e-03]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "vector = word_vectors['uncertain']\n",
    "\n",
    "print('Vector for Uncertain:')\n",
    "print(f'Length of vector: {len(vector)}')\n",
    "print(f'{vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocab List: 15,173\n",
      "['on', 'be', ';', 'I', 'by', 'had', 'at', '?', 'not', 'are', 'from', 'or', 'this', 'have', 'an', 'which', '--', 'were', 'but', 'He']\n"
     ]
    }
   ],
   "source": [
    "vocab = model.wv.key_to_index.keys()\n",
    "vocab_list = []\n",
    "for i in vocab:\n",
    "    vocab_list.append(i)\n",
    "\n",
    "print(f'Length of Vocab List: {len(vocab_list):,}')\n",
    "print(vocab_list[20:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geographic</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>singer</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beds</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>easily</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>camera</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yow</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mounts</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>duty</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>urgently</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>st</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reminded</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gabriel</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>assures</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>heightened</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>franks</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>equ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>similitude</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>laboratory</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ridge</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wearily</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Count\n",
       "0   geographic      6\n",
       "1       singer     10\n",
       "2         beds     12\n",
       "3       easily    107\n",
       "4       camera     36\n",
       "5          yow      5\n",
       "6       mounts      8\n",
       "7         duty     61\n",
       "8     urgently      6\n",
       "9           st    164\n",
       "10    reminded     29\n",
       "11     gabriel     12\n",
       "12     assures      6\n",
       "13  heightened      6\n",
       "14      franks      8\n",
       "15         equ      5\n",
       "16  similitude      6\n",
       "17  laboratory     40\n",
       "18       ridge     18\n",
       "19     wearily      7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for i in range(20:40):\n",
    "    # Pick a random word.\n",
    "    word = vocab[i]\n",
    "    count = model.wv.get_vecattr(word, 'count')\n",
    "    word_counts.append((word, count))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(word_counts, columns=['Word', 'Count'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "  Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training the model...')\n",
    "\n",
    "model.train(\n",
    "    processed_sents,\n",
    "    total_examples=len(processed_sents),\n",
    "    epochs=10,        # How many training passes to take.\n",
    "    report_delay=10.0 # Report progress every 10 seconds.\n",
    ")\n",
    "\n",
    "print('  Done.')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting: Similarity = 0.8028\n",
      "utopian: Similarity = 0.8014\n",
      "misleading: Similarity = 0.7995\n",
      "catharsis: Similarity = 0.7920\n",
      "cliche: Similarity = 0.7907\n",
      "conclude: Similarity = 0.7903\n",
      "reproduce: Similarity = 0.7857\n",
      "decisively: Similarity = 0.7846\n",
      "anymore: Similarity = 0.7838\n",
      "alienation: Similarity = 0.7787\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('uncertain', topn=10)\n",
    "\n",
    "# Print the most similar words and their similarity scores\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: Similarity = {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
